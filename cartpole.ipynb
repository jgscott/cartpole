{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import math \n",
    "\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCartPoleEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "        \n",
    "        This is just cut and pasted from the OpenAI Gym CartPole-v1 environment,\n",
    "        with a simple modification to \"sparsify\" the reward structure (see Reward\n",
    "        section below).\n",
    "\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "\n",
    "    Reward:\n",
    "        Reward is 0 for every step taken, -1 if it falls over\n",
    "\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 50\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array([self.x_threshold * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.theta_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max],\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 0.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = -1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = screen_width/world_width\n",
    "        carty = 100  # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "            axleoffset = cartheight / 4.0\n",
    "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
    "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            pole.set_color(.8, .6, .4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth/2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5, .5, .8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            self._pole_geom = pole\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        # Edit the pole polygon vertex\n",
    "        pole = self._pole_geom\n",
    "        l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
    "        pole.v = [(l, b), (l, t), (r, t), (r, b)]\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/opt/anaconda3/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# random behavior\n",
    "env = CustomCartPoleEnv()\n",
    "env.reset()\n",
    "\n",
    "for _ in range(5):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    step_counter = 0\n",
    "    sum_rewards = 0\n",
    "    for _ in range(50):\n",
    "        env.render()\n",
    "        state, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "        \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collect some statistics on states visited under random policy\n",
    "# # used for subsequent discretization of state space for tabular Q learning\n",
    "# nmc = 50000\n",
    "# saved_states = []\n",
    "\n",
    "# for i in range(nmc):\n",
    "#     env.reset()\n",
    "#     done = False\n",
    "#     step_counter = 0\n",
    "#     sum_rewards = 0\n",
    "#     while not done:\n",
    "#         state, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "#         step_counter += 1\n",
    "#         sum_rewards += reward\n",
    "#         saved_states.append(state)\n",
    "\n",
    "\n",
    "# n_points = 4\n",
    "# q = np.linspace(0,1,n_points*2)\n",
    "# cutpoints = np.quantile(saved_states, q, axis=0)\n",
    "# cutpoints = np.insert(cutpoints, n_points, 0, 0) \n",
    "\n",
    "# q = np.linspace(0.5/n_points, 1, n_points)\n",
    "# cut_pos = np.quantile(np.abs(saved_states), q, axis=0)\n",
    "# cutpoints = np.concatenate((np.flip(-cut_pos,0), cut_pos), axis=0)\n",
    "# cutpoints = np.insert(cutpoints, n_points, 0, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05 -0.38 -0.06 -0.55]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.05  0.38  0.06  0.55]]\n"
     ]
    }
   ],
   "source": [
    "cutpoints = np.array([[-0.05,0,0.05], [-0.38, 0, 0.38], [-0.06, 0, 0.06], [-0.55, 0, 0.55]]).transpose()\n",
    "\n",
    "print(cutpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple cartpole agent that implements Q learning via a tabular method.\n",
    "# In the constructor, cutpoints is a n_cuts x 4 array telling the agent how\n",
    "# to discretize the continuous 4D state space.\n",
    "\n",
    "\n",
    "class CartPoleAgent():\n",
    "    def __init__(self, env, cutpoints, epsilon = 0.1):\n",
    "        # cutpoints: an n_buckets x 4 array of cutpoints for\n",
    "        # the discrete state-space representation\n",
    "        n_buckets = cutpoints.shape[0] + 1\n",
    "        self.Q = np.zeros((n_buckets, n_buckets, n_buckets, n_buckets, 2))\n",
    "        self.action_space = env.action_space\n",
    "        self.cutpoints = cutpoints\n",
    "        self.n_buckets = n_buckets\n",
    "        \n",
    "    def discretize_state(self, state_real):\n",
    "        state_bucket = [np.digitize(state_real[i], self.cutpoints[:,i]) for i in range(4)]\n",
    "        return tuple(state_bucket)\n",
    "        \n",
    "    def choose_action(self, state, eps = 0.05):\n",
    "    # epsilon-greedy action choice\n",
    "        if(random.random() < eps):\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            state_bucket = self.discretize_state(state)\n",
    "            q_vec = self.Q[(*state_bucket),...]\n",
    "            if(q_vec[0] == q_vec[1]):\n",
    "                action = self.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(self.Q[(*state_bucket),...])\n",
    "        return action\n",
    "    \n",
    "    def show_n_episodes(self, env, n, verbose=True):\n",
    "        for i in range(n):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            step_counter = 0\n",
    "            sum_rewards = 0\n",
    "            while not done and step_counter < 500:\n",
    "                env.render(mode='human')\n",
    "                action = self.choose_action(state, eps=0) # greedy policy\n",
    "                state, reward, done, info = env.step(action)\n",
    "                sum_rewards += reward\n",
    "                step_counter += 1\n",
    "            if verbose:\n",
    "                print(step_counter)\n",
    "        env.close()\n",
    "    \n",
    "class CartPoleSarsaLambdaLearner(CartPoleAgent):\n",
    "    \n",
    "    def __init__(self, env, cutpoints, epsilon = 0.1):\n",
    "        CartPoleAgent.__init__(self, env, cutpoints, epsilon)\n",
    "        self.Z = np.zeros((self.n_buckets, self.n_buckets, self.n_buckets, self.n_buckets, 2))\n",
    "        \n",
    "    \n",
    "    def update_Q(self, SARSA, alpha=0.001, gamma=1.0, lam = 0.99):\n",
    "        current_state, action, reward, next_state, next_action = SARSA\n",
    "        \n",
    "        # discretize\n",
    "        current_state_bucket = self.discretize_state(current_state)\n",
    "        next_state_bucket = self.discretize_state(next_state)\n",
    "        \n",
    "        # reward target\n",
    "        current_Q = self.Q[(*current_state_bucket),action]\n",
    "        next_Q = self.Q[(*next_state_bucket),next_action]\n",
    "        delta = reward + gamma*next_Q - current_Q\n",
    "        \n",
    "        # update eligibility trace\n",
    "        self.Z[(*current_state_bucket),action] += 1\n",
    "        \n",
    "        # update all states and decay eligibility trace\n",
    "        self.Q += (alpha*delta)*self.Z\n",
    "        self.Z = gamma*lam*self.Z\n",
    "                \n",
    "            \n",
    "class CartPoleQLearner(CartPoleAgent):   \n",
    "    \n",
    "    def __init__(self, env, cutpoints, epsilon = 0.1):\n",
    "        CartPoleAgent.__init__(self, env, cutpoints, epsilon)\n",
    "        \n",
    "    def update_Q(self, step, alpha, gamma=1.0, method='Q'):\n",
    "        state, action, reward, next_state = step\n",
    "        state_bucket = self.discretize_state(state)\n",
    "        next_state_bucket = self.discretize_state(next_state)\n",
    "        current_Q = self.Q[(*state_bucket),action]\n",
    "        best_Qp = np.amax(self.Q[(*next_state_bucket),...])  # Q-value of best action for successor state\n",
    "        delta = reward + gamma*best_Qp - current_Q\n",
    "        self.Q[(*state_bucket),action] = current_Q + alpha*delta\n",
    "             \n",
    "            \n",
    "class CartPoleMCLearner(CartPoleAgent):   \n",
    "    \n",
    "    def __init__(self, env, cutpoints, epsilon = 0.1):\n",
    "        CartPoleAgent.__init__(self, env, cutpoints, epsilon)\n",
    "        self.C = np.zeros((self.n_buckets, self.n_buckets, self.n_buckets, self.n_buckets, 2))\n",
    "        \n",
    "    def update_Q(self, episode, alpha=0.001, gamma=1.0, method='Q'):\n",
    "        total_rewards = 0.0\n",
    "        # Simple Monte Carlo control\n",
    "        # work backwards to sum rewards and attribute those to each visited (state, action) pair in the episode\n",
    "        for state, action, reward, next_state in reversed(episode):\n",
    "            total_rewards = gamma*total_rewards + reward\n",
    "            state_bucket = self.discretize_state(state)\n",
    "            self.C[(*state_bucket),action] += 1\n",
    "            current_Q = self.Q[(*state_bucket),action]\n",
    "            current_C = self.C[(*state_bucket),action]\n",
    "            step_size =  max(alpha, 1.0/current_C)\n",
    "            self.Q[(*state_bucket),action] += step_size*(total_rewards - current_Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# SARSA(lambda) learning\n",
    "###\n",
    "\n",
    "env = CustomCartPoleEnv()\n",
    "env.reset()\n",
    "agent = CartPoleSarsaLambdaLearner(env, cutpoints)\n",
    "\n",
    "alpha_decay = 0.99\n",
    "alpha_min = 0.01\n",
    "alpha = 0.2\n",
    "lam = 0.9\n",
    "gam = 0.95\n",
    "\n",
    "\n",
    "# run some steps\n",
    "num_episodes = 100\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    eps = max(0.1, 1.0/(1.0+i))\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step_counter = 0\n",
    "    sum_rewards = 0\n",
    "    \n",
    "    # reset the eligibility trace for this episode\n",
    "    agent.Z[...] = 0.0\n",
    "    \n",
    "    # choose initial action according to epsilon-greedy version of the policy\n",
    "    action = agent.choose_action(state, eps=eps)\n",
    "    \n",
    "    while not done and step_counter < 500:\n",
    "        \n",
    "        # we know SA or SARSA from previous step\n",
    "        # now get RSA (reward, next_state, next_action)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_action = agent.choose_action(next_state, eps=eps)\n",
    "        \n",
    "        SARSA = [state, action, reward, next_state, next_action]\n",
    "        agent.update_Q(SARSA, alpha = alpha, gamma = gam, lam=lam)\n",
    "        step_counter += 1\n",
    "        sum_rewards += reward\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        \n",
    "    # decay alpha\n",
    "    alpha = max(alpha_min, alpha*alpha_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "agent.show_n_episodes(env, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0732064682546459\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Q learning\n",
    "###\n",
    "\n",
    "env = CustomCartPoleEnv()\n",
    "env.reset()\n",
    "agent = CartPoleQLearner(env, cutpoints)\n",
    "\n",
    "alpha_decay = 0.99\n",
    "alpha_min = 0.01\n",
    "alpha = 0.2\n",
    "gam = 0.95\n",
    "\n",
    "\n",
    "# run some steps\n",
    "num_episodes = 100\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    eps = max(0.1, 1.0/(1.0+i))\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step_counter = 0\n",
    "    sum_rewards = 0\n",
    "    \n",
    "    while not done and step_counter < 500:\n",
    "        \n",
    "        action = agent.choose_action(state, eps=eps)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "        agent.update_Q([state, action, reward, next_state], alpha = alpha, gamma = gam)\n",
    "        step_counter += 1\n",
    "        sum_rewards += reward\n",
    "        state = next_state\n",
    "        \n",
    "    # decay alpha\n",
    "    alpha = max(alpha_min, alpha*alpha_decay)\n",
    "\n",
    "print(alpha)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "163\n",
      "163\n"
     ]
    }
   ],
   "source": [
    "agent.show_n_episodes(env, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0732064682546459\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Monte Carlo learning\n",
    "###\n",
    "\n",
    "env = CustomCartPoleEnv()\n",
    "env.reset()\n",
    "agent = CartPoleMCLearner(env, cutpoints)\n",
    "\n",
    "alpha_decay = 0.99\n",
    "alpha_min = 0.01\n",
    "alpha = 0.2\n",
    "lam = 0.8\n",
    "gam = 0.99\n",
    "\n",
    "\n",
    "# run some steps\n",
    "num_episodes = 100\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    eps = max(0.1, 1.0/(1.0+i))\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step_counter = 0\n",
    "    sum_rewards = 0\n",
    "    episode = []\n",
    "    \n",
    "    while not done and step_counter < 500:\n",
    "        \n",
    "        action = agent.choose_action(state, eps=eps)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        step_counter += 1\n",
    "        sum_rewards += reward\n",
    "        episode.append([state, action, reward, next_state])\n",
    "        state = next_state\n",
    "        \n",
    "        \n",
    "    # decay alpha\n",
    "    agent.update_Q(episode, alpha = alpha, gamma = gam)\n",
    "    alpha = max(alpha_min, alpha*alpha_decay)\n",
    "\n",
    "print(alpha)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "466\n",
      "401\n"
     ]
    }
   ],
   "source": [
    "agent.show_n_episodes(env, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
